{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51191529-88e7-4464-8ca5-73328ab5786f",
   "metadata": {},
   "source": [
    "# Project 3: Differentiate Reddit Biotech and Bioinformatics Subreddits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20c304-dd86-42f8-829e-7a01bb2228a7",
   "metadata": {},
   "source": [
    "Create a model to differentiate Reddit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7bb3cbf-ce7f-43e9-857a-c1b22233639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293eea5a-ab0b-4133-bc0c-8bbb8c5ad210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to the Reddit \n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbc59e-53ec-4205-a4d4-a8aab3ad7c79",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b158c93-47f8-4b9e-a79f-4afabf8dbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit, limit, until = None):\n",
    "    '''\n",
    "    Input:\n",
    "    Subreddit name, \n",
    "    Limit of the input, \n",
    "    Until - default None\n",
    "    \n",
    "    Output: the Data Frame with responce \n",
    "    '''\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'limit': limit,\n",
    "        'filter': 'subreddit, selftext,title, created_utc',\n",
    "        'until': until\n",
    "    }\n",
    "\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "\n",
    "    print(f'Status code {res.status_code}')\n",
    "    \n",
    "    return pd.DataFrame(res.json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2510658c-b698-41cb-9c90-189fdab7225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_n_times(n, subreddit, limit):\n",
    "    '''\n",
    "    Input:\n",
    "    Number of times to get data\n",
    "    Subreddit name\n",
    "    Limit per request   \n",
    "    \n",
    "    Output Data Frame with all collected data\n",
    "    '''\n",
    "    data = get_subreddit_data(subreddit, limit, until = None)\n",
    "    last_created_utc = data['created_utc'].tail(1)\n",
    "    for _ in range(n-1):\n",
    "        data = pd.concat([data, get_subreddit_data(subreddit, limit, until = last_created_utc)], ignore_index=True)\n",
    "        last_created_utc = data['created_utc'].tail(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bd31b-5f6a-45ff-8718-b35310a9c0ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7773f-4c1b-4f8c-b279-993fa884558b",
   "metadata": {},
   "source": [
    "# Collecting the Data from Reddit using PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7778a077-c2f6-4aed-8c46-c30afb607e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n"
     ]
    }
   ],
   "source": [
    "# Collecting data from the Biology subreddit\n",
    "bioinformatics = get_data_n_times(8, 'bioinformatics', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065a4bbd-4015-41d7-a5f5-15491769db75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the amount of data available \n",
    "bioinformatics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af68c00c-5536-4508-8a50-e7bf52986148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n"
     ]
    }
   ],
   "source": [
    "# Collecting data from the Health subreddit\n",
    "biotech = get_data_n_times(8, 'biotech', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4214193f-f296-46e8-b35c-b244c8e90738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(795, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the amount of data available \n",
    "biotech.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f9468bc-d8aa-48d2-b7fe-a882a05335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conmine data from two reddits\n",
    "reddit = pd.concat([bioinformatics, biotech], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb3a4d57-5176-4ded-975d-ee2b3a5e9fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1449, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c5b1b2-0dd3-4156-99d6-955bfd551c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to file\n",
    "reddit.to_csv('../data/reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18405183-6332-4b16-9758-1ddacac05b27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb680ef4-ff99-499e-a39c-d6594d2a0eae",
   "metadata": {},
   "source": [
    "I gathered about the same ammount of data for both subreddits.\n",
    "Next I will clean and prepare the data for modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
