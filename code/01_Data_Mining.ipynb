{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51191529-88e7-4464-8ca5-73328ab5786f",
   "metadata": {},
   "source": [
    "# Project 3: Differentiate Reddit Bioinformatics and Data Science Subreddits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20c304-dd86-42f8-829e-7a01bb2228a7",
   "metadata": {},
   "source": [
    "In this project, I will create a model to differentiate Bioinformatics and Data Science-related articles. I will focus on determining Reddit Bioinformatics and Data Science subreddits in this project. My baseline is 67%. I have imbalanced classes and will use F1 as my primary metric and accuracy score as a helper to find the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ead77ba-d43e-4c5a-bb67-182c2be9f792",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f11f7-6848-475b-8f36-db23694ea3d2",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbd85a-6e0b-4460-8a36-9146b70f264f",
   "metadata": {},
   "source": [
    "In this notebook, I will collect data from Reddit Bioinformatics and Data Science subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7bb3cbf-ce7f-43e9-857a-c1b22233639b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293eea5a-ab0b-4133-bc0c-8bbb8c5ad210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to the Reddit \n",
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dbc59e-53ec-4205-a4d4-a8aab3ad7c79",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b158c93-47f8-4b9e-a79f-4afabf8dbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddit_data(subreddit, limit, until = None):\n",
    "    '''\n",
    "    Input:\n",
    "    Subreddit name, \n",
    "    Limit of the input, \n",
    "    Until - default None\n",
    "    \n",
    "    Output: the Data Frame with response \n",
    "    '''\n",
    "    params = {\n",
    "        'subreddit': subreddit,\n",
    "        'limit': limit,\n",
    "        'filter': 'subreddit, selftext,title, created_utc',\n",
    "        'until': until\n",
    "    }\n",
    "\n",
    "\n",
    "    res = requests.get(url, params)\n",
    "\n",
    "    print(f'Status code {res.status_code}')\n",
    "    \n",
    "    return pd.DataFrame(res.json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2510658c-b698-41cb-9c90-189fdab7225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_n_times(n, subreddit, limit):\n",
    "    '''\n",
    "    Input:\n",
    "    Number of times to get data\n",
    "    Subreddit name\n",
    "    Limit per request   \n",
    "    \n",
    "    Output Data Frame with all collected data\n",
    "    '''\n",
    "    data = get_subreddit_data(subreddit, limit, until = None)\n",
    "    last_created_utc = data['created_utc'].tail(1)\n",
    "    for _ in range(n-1):\n",
    "        data = pd.concat([data, get_subreddit_data(subreddit, limit, until = last_created_utc)], ignore_index=True)\n",
    "        last_created_utc = data['created_utc'].tail(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118bd31b-5f6a-45ff-8718-b35310a9c0ff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7773f-4c1b-4f8c-b279-993fa884558b",
   "metadata": {},
   "source": [
    "## Collecting the Data from Reddit using PushShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7778a077-c2f6-4aed-8c46-c30afb607e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n"
     ]
    }
   ],
   "source": [
    "# Collecting data from the Bioinformatics subreddit\n",
    "bioinformatics = get_data_n_times(7, 'bioinformatics', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065a4bbd-4015-41d7-a5f5-15491769db75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the amount of data available \n",
    "bioinformatics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f486c0c-80e6-45c5-b7d1-6d0cec877370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n",
      "Status code 200\n"
     ]
    }
   ],
   "source": [
    "# Collecting data from the Data Science subreddit\n",
    "datascience = get_data_n_times(14, 'datascience', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4214193f-f296-46e8-b35c-b244c8e90738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1400, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the amount of data available \n",
    "datascience.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f9468bc-d8aa-48d2-b7fe-a882a05335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data from two reddits\n",
    "reddit = pd.concat([bioinformatics, datascience], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb3a4d57-5176-4ded-975d-ee2b3a5e9fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2083, 4)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67c5b1b2-0dd3-4156-99d6-955bfd551c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save collected data to file\n",
    "reddit.to_csv('../data/reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18405183-6332-4b16-9758-1ddacac05b27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb680ef4-ff99-499e-a39c-d6594d2a0eae",
   "metadata": {},
   "source": [
    "I gathered enough data for both subreddits. Next, I will clean, explore and prepare the data for modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
